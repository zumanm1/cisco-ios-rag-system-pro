# Cisco IOS RAG System Pro: Detailed Output and Workflow Explanation

This document provides a comprehensive overview of the output generated by the "Cisco IOS RAG System Pro" application, the input it processes, and a detailed explanation of the script's inner workings from source to final output.

## 1. About the Output

The application generates a structured set of knowledge libraries derived from a Cisco IOS command guide PDF. The primary purpose of this output is to create high-quality, fine-tuning datasets for a Retrieval-Augmented Generation (RAG) system. By structuring the information into distinct categories, the RAG system can provide more accurate, context-aware, and helpful responses to network engineering queries.

Each run of the application creates a unique session folder inside the `OUTPUT-FOLDER` directory. This folder is timestamped to ensure that results from different runs are kept separate.

The full path for the last run is:
`/home/bootssd-2t/Documents/GENAI-44-C-FINE-TUNE/OUTPUT-FOLDER/session_20250712_003511`

The session folder contains the following files and directories:
- **`knowledge_libraries/`**
  - `best_practices/best_practices_library.json`: A collection of best practices.
  - `error_patterns/error_patterns_library.json`: A set of common error patterns.
- **`logs/`**
  - `processing.log`: Records the application's activity.
- **`config/`**
  - `session_config.json`: The configuration used for the session.
- **`vector_db/`**: Contains the Chroma vector store persistence.
- Other directories for intermediate processing (`checkpoints`, `chunks`, `extracted`, `reports`).


## 2. Input Data

The primary input for the application is a **Cisco IOS Command Guide in PDF format**. The user uploads this document through the Streamlit interface. The application is designed to process text-based PDFs containing technical documentation about network commands, configurations, and operational procedures.

## 3. Output Data Explained

The generated JSON files serve as structured datasets for fine-tuning a Large Language Model (LLM). Each file targets a specific area of knowledge to improve the RAG system's performance:

- **`error_patterns_library.json`**: This library contains examples of incorrect command usage or common misconfigurations. Each entry includes the problematic command and a description of why it is wrong and how to fix it. This helps the RAG system recognize and correct user errors.

- **`best_practices_library.json`**: This file includes recommended methods for using commands or configuring network features. It helps the RAG system offer advice that is not just functional but also aligns with industry best practices for security, performance, and maintainability.

- **`command_syntax_library.json`**: This provides a clean, structured reference for command syntax. The RAG system can use this to provide quick and accurate syntax lookups for users. This was not generated in the last run based on the output.

- **`troubleshooting_scenarios_library.json`**: This library contains "what-if" scenarios and step-by-step troubleshooting guides. It equips the RAG system to help users diagnose and resolve network issues effectively. This was not generated in the last run based on the output.

## 4. Inner Workings of the Script (`app.py`)

The application is built using Python and Streamlit, with several classes working together to manage the processing pipeline. Here is a breakdown of the key components and their interactions:

### Core Components:

- **`AppState`**: A dataclass that holds the entire state of the application, including configuration, progress, system metrics, and logs. This centralized state is passed to different components, ensuring data consistency, especially in a multi-threaded environment.

- **`OutputManager`**: Responsible for all file system operations. It creates the main `OUTPUT-FOLDER` and a unique, timestamped session directory for each run. It also handles the saving of the generated JSON libraries and log files.

- **`KnowledgeBuilder`**: The core component for data generation. It takes the text extracted from the input PDF and uses a language model (via LangChain and Ollama) to generate the content for each knowledge library. It uses a vector store (Chroma) to perform semantic searches on the document, finding relevant context to generate varied and accurate examples for each category.

- **`ProgressTracker`**: Manages and displays the progress of the data generation pipeline in the Streamlit UI. It updates progress bars and status messages in real-time. A `threading.Lock` is used to prevent race conditions when updating the application state from background threads.

### Workflow from Source to Output:

1.  **Initialization**: When the app starts, it initializes the `AppState` in the Streamlit session state (`st.session_state`). This object will track everything for the current user session.

2.  **UI and Configuration**: The Streamlit interface presents two main tabs: "Configuration & Processing" and "Fine-Tuning Guide."
    - In the **Configuration** tab, the user uploads a PDF and sets parameters like the number of examples to generate for each library using sliders.
    - The **Fine-Tuning Guide** tab provides hardware-based recommendations for model fine-tuning.

3.  **Processing Pipeline**:
    - When the user clicks "Start Processing," the `main()` function kicks off the pipeline.
    - An `OutputManager` is created to set up the session directory for the current run.
    - A `ProgressTracker` is initialized to monitor and display progress.
    - The core processing is handed off to two background threads to keep the UI responsive:
        - One thread runs the main `processing_pipeline` function.
        - Another thread runs `monitor_system_resources` to track CPU and memory usage.

4.  **Data Extraction and Generation**:
    - Inside the `processing_pipeline`, the uploaded PDF is read, and its text is extracted.
    - A `KnowledgeBuilder` instance is created. It chunks the text, creates embeddings using an Ollama model, and stores them in a Chroma vector database.
    - The `KnowledgeBuilder` then iteratively generates each knowledge library (error patterns, best practices, etc.). For each library, it queries the vector store to find relevant information and uses an LLM to generate the structured JSON output based on predefined prompts.
    - As each library is generated, the `ProgressTracker` updates the UI with the current status.

5.  **Output and Logging**:
    - The generated JSON data for each library is saved to the session folder by the `OutputManager`.
    - All processing events, including progress updates and errors, are logged to `processing.log` within the session's `logs` directory.

6.  **Completion**: Once all libraries are generated, the processing thread finishes, the progress bars in the UI show 100%, and the user can find all the generated files in the designated output directory. 